version: "3.8"

services:
  # --- Nautobot for Network Automation ---
  nautobot:
    image: networktocode/nautobot:latest
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1'
          memory: 1G
    volumes:
      - type: bind
        source: ${PWD}/config/nautobot_config.py
        target: /opt/nautobot/nautobot_config.py
      - type: bind
        source: ${PWD}/config/nautobot_plugin.yml
        target: /opt/nautobot/plugins.yml
    environment:
      - NAUTOBOT_DB_HOST=nautobot-db
      - NAUTOBOT_DB_PORT=5432
      - NAUTOBOT_DB_NAME=nautobot
      - NAUTOBOT_DB_USER=nautobot
      - NAUTOBOT_DB_PASSWORD=nautobot
    ports:
      - "8001:8000"
    depends_on:
      - nautobot-db
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/", "||", "exit", "0"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - monitoring

  nautobot-db:
    image: postgres:14
    environment:
      - POSTGRES_DB=nautobot
      - POSTGRES_USER=nautobot
      - POSTGRES_PASSWORD=nautobot
    volumes:
      - nautobot-db-data:/var/lib/postgresql/data
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "nautobot"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - monitoring

  # --- Prometheus for Metrics Collection ---
  prometheus:
    image: prom/prometheus:latest
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.retention.time=15d"
      - "--web.enable-lifecycle"
    volumes:
      - type: bind
        source: ${PWD}/config/prometheus.yml
        target: /etc/prometheus/prometheus.yml
      - type: bind
        source: ${PWD}/config/prometheus-alerts.yml
        target: /etc/prometheus/prometheus-alerts.yml
      - prometheus-data:/prometheus
    ports:
      - "9091:9090"
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:9090/-/healthy", "||", "exit", "0"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - monitoring

  # --- Alertmanager ---
  alertmanager:
    image: prom/alertmanager:latest
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
    ports:
      - "9094:9093"
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.2'
          memory: 128M
    volumes:
      - type: bind
        source: ${PWD}/config/alertmanager.yml
        target: /etc/alertmanager/alertmanager.yml
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:9093/-/healthy", "||", "exit", "0"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - monitoring

  # --- Node Exporter for Host Metrics ---
  node-exporter:
    image: prom/node-exporter:latest
    command:
      - "--path.rootfs=/host"
      - "--web.max-requests=5"
      - "--web.disable-exporter-metrics"
      - "--web.telemetry-path=/minimal-metrics"
      - "--collector.disable-defaults"
      - "--collector.cpu"
      - "--collector.meminfo"
      - "--collector.loadavg"
    volumes:
      - /:/host:ro,rslave
    ports:
      - "9101:9100"
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.1'
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "--timeout=10", "--tries=1", "--spider", "http://localhost:9100/", "||", "exit", "0"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - monitoring

  # --- Loki for Log Aggregation ---
  loki:
    image: grafana/loki:latest
    command: -config.file=/etc/loki/loki-config.yml
    volumes:
      - type: bind
        source: ${PWD}/config/loki-config.yml
        target: /etc/loki/loki-config.yml
      - loki-data:/loki
    ports:
      - "3101:3100"
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:3100/ready", "||", "exit", "0"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - monitoring

  # --- Logstash for Log Collection ---
  logstash:
    image: docker.elastic.co/logstash/logstash:7.16.2
    volumes:
      - type: bind
        source: ${PWD}/config/logstash.conf
        target: /usr/share/logstash/pipeline/logstash.conf
    ports:
      - "514:514/udp"
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    depends_on:
      - loki
    healthcheck:
      test: ["CMD", "curl", "--silent", "--fail", "localhost:9600/_node/stats", "||", "exit", "0"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - monitoring
      
  # --- Telegraf for Metrics Collection ---
  telegraf:
    image: telegraf:latest
    volumes:
      - type: bind
        source: ${PWD}/config/telegraf.conf
        target: /etc/telegraf/telegraf.conf
        read_only: true
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        read_only: true
    ports:
      - "8125:8125/udp"
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.3'
          memory: 256M
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "telegraf", "--test", "||", "exit", "0"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - monitoring

  # --- Grafana for Visualization ---
  grafana:
    image: grafana/grafana:latest
    volumes:
      - type: bind
        source: ${PWD}/config/grafana-datasources.yml
        target: /etc/grafana/provisioning/datasources/datasources.yml
      - type: bind
        source: ${PWD}/config/grafana-dashboards.yml
        target: /etc/grafana/provisioning/dashboards/dashboards.yml
      - type: bind
        source: ${PWD}/dashboards/network
        target: /etc/grafana/dashboards/network
      - type: bind
        source: ${PWD}/dashboards/system
        target: /etc/grafana/dashboards/system
      - type: bind
        source: ${PWD}/dashboards/logs
        target: /etc/grafana/dashboards/logs
      - grafana-data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3001:3000"
    deploy:
      replicas: 1
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.3'
          memory: 256M
    depends_on:
      - prometheus
      - loki
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:3000/api/health", "||", "exit", "0"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - monitoring

networks:
  monitoring:
    driver: overlay

volumes:
  nautobot-db-data:
    driver: local
  prometheus-data:
    driver: local
  loki-data:
    driver: local
  grafana-data:
    driver: local